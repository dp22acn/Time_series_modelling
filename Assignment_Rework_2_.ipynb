{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNawzKj3Q5wbRfQrCvFOetn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dp22acn/Time_series_modelling/blob/main/Assignment_Rework_2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Making Initial Plots**"
      ],
      "metadata": {
        "id": "heIuM7aCzTbe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NhFzxS04KBp"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "import os\n",
        "import pathlib\n",
        "from google.colab import drive\n",
        "\n",
        "# Install pmdarima\n",
        "!pip install pmdarima\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.rcParams['figure.figsize'] = [10, 7.5]\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load Johnson & Johnson dataset\n",
        "file_path = \"/content/drive/MyDrive/jj.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows and data information\n",
        "print(data.head())\n",
        "print(data.info())\n",
        "\n",
        "# Plot Johnson & Johnson Quarterly Sales\n",
        "data['data'].plot(title='Johnson & Johnson Quarterly Sales', figsize=(10, 6))\n",
        "plt.ylabel('Sales')\n",
        "plt.xlabel('Year')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Crl9m-_psHyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perform all the time series analysis tasks to test for non-stationarity**"
      ],
      "metadata": {
        "id": "hIj6ljE9z3rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Auto Correlation Function (ACF) and Partial Auto Correlation Function (PACF)\n",
        "plot_acf(data['data'], color='green')\n",
        "plt.title('Auto Correlation Function (ACF)')\n",
        "plt.xlabel('Lags')\n",
        "plt.ylabel('ACF')\n",
        "plt.show()\n",
        "\n",
        "plot_pacf(data['data'], color='green')\n",
        "plt.title('Partial Auto Correlation Function (PACF)')\n",
        "plt.xlabel('Lags')\n",
        "plt.ylabel('PACF')\n",
        "plt.show()\n",
        "\n",
        "# Augmented Dickey-Fuller Test (ADF) for stationarity check\n",
        "result = adfuller(data['data'])\n",
        "print('ADF Statistic:', result[0])\n",
        "print('p-value:', result[1])\n",
        "print('Critical Values:')\n",
        "for key, value in result[4].items():\n",
        "    print('\\t%s: %.3f' % (key, value))\n"
      ],
      "metadata": {
        "id": "_CnIupu9z3R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Make the data stationary**"
      ],
      "metadata": {
        "id": "EdCQhke-0Flw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the logarithm of the data and difference it to make it stationary\n",
        "data['log_data'] = np.log(data['data'])\n",
        "data['diff_log_data'] = data['log_data'].diff().dropna()\n",
        "\n",
        "# Plot the differenced data\n",
        "data['diff_log_data'].plot(title='Differenced Logarithm of Johnson & Johnson Sales', figsize=(10, 6))\n",
        "plt.ylabel('Differenced Log Sales')\n",
        "plt.xlabel('Year')\n",
        "plt.show()\n",
        "\n",
        "# Check stationarity again with ADF test\n",
        "result = adfuller(data['diff_log_data'].dropna())\n",
        "print('ADF Statistic (Differenced Log):', result[0])\n",
        "print('p-value:', result[1])\n",
        "print('Critical Values:')\n",
        "for key, value in result[4].items():\n",
        "    print('\\t%s: %.3f' % (key, value))\n"
      ],
      "metadata": {
        "id": "94ToZqnm0IzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define an ARMA model**"
      ],
      "metadata": {
        "id": "6Sob5Evd0aCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import pmdarima as pm\n",
        "\n",
        "# Define the ARMA model with pmdarima to find best parameters\n",
        "auto_model = pm.auto_arima(data['diff_log_data'].dropna(), start_p=0, start_q=0,\n",
        "                           max_p=8, max_q=8, seasonal=False,\n",
        "                           d=0, trace=True, error_action='ignore',\n",
        "                           suppress_warnings=True, stepwise=True)\n",
        "\n",
        "print(\"Best AIC (Auto):\", auto_model.aic())\n",
        "print(auto_model.summary())\n"
      ],
      "metadata": {
        "id": "jwR3wPTW0ZsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Find the best model parameters: p,d,q**"
      ],
      "metadata": {
        "id": "ks3kommb0ogJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual approach to find the best SARIMA model\n",
        "min_aic = float('inf')\n",
        "best_order = None\n",
        "\n",
        "for p, d, q in product(range(9), range(2), range(9)):\n",
        "    try:\n",
        "        model = SARIMAX(data['data'], order=(p, d, q))\n",
        "        results = model.fit(disp=False)\n",
        "        if results.aic < min_aic:\n",
        "            min_aic = results.aic\n",
        "            best_order = (p, d, q)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(\"Best AIC (Manual):\", min_aic)\n",
        "print(\"Best Order (Manual):\", best_order)\n"
      ],
      "metadata": {
        "id": "lGOtoT3v0oBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test model performance**"
      ],
      "metadata": {
        "id": "wDDXngbq1SBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and fit the best SARIMA model based on manual approach\n",
        "best_model = SARIMAX(data['data'], order=best_order)\n",
        "best_model_fit = best_model.fit()\n",
        "\n",
        "# Display the summary of the best model\n",
        "print(\"Best Model Summary:\")\n",
        "print(best_model_fit.summary())\n",
        "\n",
        "# Plot diagnostics for the best model\n",
        "best_model_fit.plot_diagnostics()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NK9lvLjU1RSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Forecast model 24 months into the future**"
      ],
      "metadata": {
        "id": "wspTOPO61w2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forecast for the next 24 months\n",
        "n_forecast = 24\n",
        "forecast_result = best_model_fit.get_forecast(steps=n_forecast)\n",
        "\n",
        "# Get forecasted mean and confidence intervals\n",
        "forecasted_mean = forecast_result.predicted_mean\n",
        "confidence_intervals = forecast_result.conf_int()\n",
        "\n",
        "# Create a new index for the forecasted values\n",
        "forecast_index = np.arange(len(data['data']), len(data['data']) + n_forecast)\n",
        "\n",
        "# Plot the actual data, forecasted values, and confidence intervals\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(data['data'], label='Actual Sales', color='blue')\n",
        "plt.plot(forecast_index, forecasted_mean, label='Forecast', color='orange')\n",
        "plt.fill_between(forecast_index, confidence_intervals.iloc[:, 0], confidence_intervals.iloc[:, 1], color='green', alpha=0.5, label='Confidence Interval')\n",
        "plt.title('Forecast of Johnson & Johnson Sales with Confidence Intervals')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "77GBal5e1wcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build an RNN based model (LSTM, GRU)**"
      ],
      "metadata": {
        "id": "Y15i3Pme2W7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load Johnson & Johnson dataset\n",
        "file_path = \"/content/drive/MyDrive/jj.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocess data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data['data'].values.reshape(-1, 1))\n",
        "\n",
        "# Split data into train and test sets (70-30 split)\n",
        "train_size = int(len(scaled_data) * 0.7)\n",
        "train_data, test_data = scaled_data[:train_size], scaled_data[train_size:]\n",
        "\n",
        "# Define function to create sequences\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Define sequence length\n",
        "seq_length = 12\n",
        "\n",
        "# Create sequences for training and testing data\n",
        "X_train, y_train = create_sequences(train_data, seq_length)\n",
        "X_test, y_test = create_sequences(test_data, seq_length)\n",
        "\n",
        "# Build the GRU model\n",
        "model = Sequential([\n",
        "    GRU(64, activation='relu', input_shape=(seq_length, 1)),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Forecasting\n",
        "train_forecast = model.predict(X_train)\n",
        "test_forecast = model.predict(X_test)\n",
        "\n",
        "# Inverse transform forecasted values\n",
        "train_forecast = scaler.inverse_transform(train_forecast).flatten()\n",
        "test_forecast = scaler.inverse_transform(test_forecast).flatten()\n",
        "y_train = scaler.inverse_transform(y_train).flatten()\n",
        "y_test = scaler.inverse_transform(y_test).flatten()\n",
        "\n",
        "# Calculate RMSE\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, train_forecast))\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, test_forecast))\n",
        "print(f'Train RMSE: {train_rmse}')\n",
        "print(f'Test RMSE: {test_rmse}')\n",
        "\n",
        "# Plotting the forecast\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot actual train sales\n",
        "plt.plot(data.index[seq_length:seq_length + len(train_forecast)], y_train, label='Actual Train Sales', color='blue')\n",
        "\n",
        "# Plot actual test sales (starting after the train data)\n",
        "plt.plot(data.index[train_size + seq_length:], y_test, label='Actual Test Sales', color='green')\n",
        "\n",
        "# Plot train forecast\n",
        "plt.plot(data.index[seq_length:seq_length + len(train_forecast)], train_forecast, label='Train Forecast', color='red', linestyle='--')\n",
        "\n",
        "# Plot test forecast (starting after the train data)\n",
        "plt.plot(data.index[train_size + seq_length:], test_forecast, label='Test Forecast', color='orange', linestyle='--')\n",
        "\n",
        "plt.title('Forecast of Johnson & Johnson Sales with GRU')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qwJSWNUa2WhJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}