{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Making Initial Plots**"
      ],
      "metadata": {
        "id": "heIuM7aCzTbe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NhFzxS04KBp"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "import os\n",
        "import pathlib\n",
        "from google.colab import drive\n",
        "from itertools import product\n",
        "\n",
        "# Install pmdarima\n",
        "!pip install pmdarima\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.rcParams['figure.figsize'] = [10, 7.5]\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load Johnson & Johnson dataset\n",
        "file_path = \"/content/drive/MyDrive/jj.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows and data information\n",
        "print(data.head())\n",
        "print(data.info())\n",
        "\n",
        "# Plot Johnson & Johnson Quarterly Sales\n",
        "data['data'].plot(title='Johnson & Johnson Quarterly Sales', figsize=(10, 6))\n",
        "plt.ylabel('Sales')\n",
        "plt.xlabel('Year')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Crl9m-_psHyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perform all the time series analysis tasks to test for non-stationarity**"
      ],
      "metadata": {
        "id": "hIj6ljE9z3rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Auto Correlation Function (ACF) and Partial Auto Correlation Function (PACF)\n",
        "plot_acf(data['data'], color='green')\n",
        "plt.title('Auto Correlation Function (ACF)')\n",
        "plt.xlabel('Lags')\n",
        "plt.ylabel('ACF')\n",
        "plt.show()\n",
        "\n",
        "plot_pacf(data['data'], color='green')\n",
        "plt.title('Partial Auto Correlation Function (PACF)')\n",
        "plt.xlabel('Lags')\n",
        "plt.ylabel('PACF')\n",
        "plt.show()\n",
        "\n",
        "# Augmented Dickey-Fuller Test (ADF) for stationarity check\n",
        "result = adfuller(data['data'])\n",
        "print('ADF Statistic:', result[0])\n",
        "print('p-value:', result[1])\n",
        "print('Critical Values:')\n",
        "for key, value in result[4].items():\n",
        "    print('\\t%s: %.3f' % (key, value))\n"
      ],
      "metadata": {
        "id": "_CnIupu9z3R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Make the data stationary**"
      ],
      "metadata": {
        "id": "EdCQhke-0Flw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the logarithm of the data and difference it to make it stationary\n",
        "data['log_data'] = np.log(data['data'])\n",
        "data['diff_log_data'] = data['log_data'].diff().dropna()\n",
        "\n",
        "# Plot the differenced data\n",
        "data['diff_log_data'].plot(title='Differenced Logarithm of Johnson & Johnson Sales', figsize=(10, 6))\n",
        "plt.ylabel('Differenced Log Sales')\n",
        "plt.xlabel('Year')\n",
        "plt.show()\n",
        "\n",
        "# Check stationarity again with ADF test\n",
        "result = adfuller(data['diff_log_data'].dropna())\n",
        "print('ADF Statistic (Differenced Log):', result[0])\n",
        "print('p-value:', result[1])\n",
        "print('Critical Values:')\n",
        "for key, value in result[4].items():\n",
        "    print('\\t%s: %.3f' % (key, value))\n"
      ],
      "metadata": {
        "id": "94ToZqnm0IzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define an ARMA model**"
      ],
      "metadata": {
        "id": "6Sob5Evd0aCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import pmdarima as pm\n",
        "\n",
        "# Define the ARMA model with pmdarima to find best parameters\n",
        "auto_model = pm.auto_arima(data['diff_log_data'].dropna(), start_p=0, start_q=0,\n",
        "                           max_p=8, max_q=8, seasonal=False,\n",
        "                           d=0, trace=True, error_action='ignore',\n",
        "                           suppress_warnings=True, stepwise=True)\n",
        "\n",
        "print(\"Best AIC (Auto):\", auto_model.aic())\n",
        "print(auto_model.summary())\n"
      ],
      "metadata": {
        "id": "jwR3wPTW0ZsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Find the best model parameters: p,d,q**"
      ],
      "metadata": {
        "id": "ks3kommb0ogJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual approach to find the best SARIMA model\n",
        "min_aic = float('inf')\n",
        "best_order = None\n",
        "\n",
        "for p, d, q in product(range(9), range(2), range(9)):\n",
        "    try:\n",
        "        model = SARIMAX(data['data'], order=(p, d, q))\n",
        "        results = model.fit(disp=False)\n",
        "        if results.aic < min_aic:\n",
        "            min_aic = results.aic\n",
        "            best_order = (p, d, q)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(\"Best AIC (Manual):\", min_aic)\n",
        "print(\"Best Order (Manual):\", best_order)\n"
      ],
      "metadata": {
        "id": "lGOtoT3v0oBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test model performance**"
      ],
      "metadata": {
        "id": "wDDXngbq1SBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and fit the best SARIMA model based on manual approach\n",
        "best_model = SARIMAX(data['data'], order=best_order)\n",
        "best_model_fit = best_model.fit()\n",
        "\n",
        "# Display the summary of the best model\n",
        "print(\"Best Model Summary:\")\n",
        "print(best_model_fit.summary())\n",
        "\n",
        "# Plot diagnostics for the best model\n",
        "best_model_fit.plot_diagnostics()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NK9lvLjU1RSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Forecast model 24 months into the future**"
      ],
      "metadata": {
        "id": "wspTOPO61w2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forecast for the next 24 months\n",
        "n_forecast = 24\n",
        "forecast_result = best_model_fit.get_forecast(steps=n_forecast)\n",
        "\n",
        "# Get forecasted mean and confidence intervals\n",
        "forecasted_mean = forecast_result.predicted_mean\n",
        "confidence_intervals = forecast_result.conf_int()\n",
        "\n",
        "# Create a new index for the forecasted values\n",
        "forecast_index = np.arange(len(data['data']), len(data['data']) + n_forecast)\n",
        "\n",
        "# Plot the actual data, forecasted values, and confidence intervals\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(data['data'], label='Actual Sales', color='blue')\n",
        "plt.plot(forecast_index, forecasted_mean, label='Forecast', color='orange')\n",
        "plt.fill_between(forecast_index, confidence_intervals.iloc[:, 0], confidence_intervals.iloc[:, 1], color='green', alpha=0.5, label='Confidence Interval')\n",
        "plt.title('Forecast of Johnson & Johnson Sales with Confidence Intervals')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "77GBal5e1wcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build an RNN based model (LSTM, GRU)**"
      ],
      "metadata": {
        "id": "Y15i3Pme2W7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load Johnson & Johnson dataset\n",
        "file_path = \"/content/drive/MyDrive/jj.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocess data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data['data'].values.reshape(-1, 1))\n",
        "\n",
        "# Split data into train and test sets (70-30 split)\n",
        "train_size = int(len(scaled_data) * 0.7)\n",
        "train_data, test_data = scaled_data[:train_size], scaled_data[train_size:]\n",
        "\n",
        "# Define function to create sequences\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Define sequence length\n",
        "seq_length = 12\n",
        "\n",
        "# Create sequences for training and testing data\n",
        "X_train, y_train = create_sequences(train_data, seq_length)\n",
        "X_test, y_test = create_sequences(test_data, seq_length)\n",
        "\n",
        "# Build the GRU model\n",
        "model = Sequential([\n",
        "    GRU(64, activation='relu', input_shape=(seq_length, 1)),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Forecasting\n",
        "train_forecast = model.predict(X_train)\n",
        "test_forecast = model.predict(X_test)\n",
        "\n",
        "# Inverse transform forecasted values\n",
        "train_forecast = scaler.inverse_transform(train_forecast).flatten()\n",
        "test_forecast = scaler.inverse_transform(test_forecast).flatten()\n",
        "y_train = scaler.inverse_transform(y_train).flatten()\n",
        "y_test = scaler.inverse_transform(y_test).flatten()\n",
        "\n",
        "# Calculate RMSE\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, train_forecast))\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, test_forecast))\n",
        "print(f'Train RMSE: {train_rmse}')\n",
        "print(f'Test RMSE: {test_rmse}')\n",
        "\n",
        "# Plotting the forecast\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot actual train sales\n",
        "plt.plot(data.index[seq_length:seq_length + len(train_forecast)], y_train, label='Actual Train Sales', color='blue')\n",
        "\n",
        "# Plot actual test sales (starting after the train data)\n",
        "plt.plot(data.index[train_size + seq_length:], y_test, label='Actual Test Sales', color='green')\n",
        "\n",
        "# Plot train forecast\n",
        "plt.plot(data.index[seq_length:seq_length + len(train_forecast)], train_forecast, label='Train Forecast', color='red', linestyle='--')\n",
        "\n",
        "# Plot test forecast (starting after the train data)\n",
        "plt.plot(data.index[train_size + seq_length:], test_forecast, label='Test Forecast', color='orange', linestyle='--')\n",
        "\n",
        "plt.title('Forecast of Johnson & Johnson Sales with GRU')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qwJSWNUa2WhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM**\n"
      ],
      "metadata": {
        "id": "UeA7p7ac8rEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Load data\n",
        "file_path = \"/content/drive/MyDrive/jj.csv\"\n",
        "data = pd.read_csv(file_path, parse_dates=['date'])\n",
        "data.sort_values(by='date', inplace=True)\n",
        "\n",
        "# Scale the data\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data['data'].values.reshape(-1, 1))\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:(i + seq_length), 0])\n",
        "        y.append(data[i + seq_length, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 12\n",
        "X_data, y_data = create_sequences(data_scaled, seq_length)\n",
        "\n",
        "# Reshape data for LSTM\n",
        "X_data = X_data.reshape((X_data.shape[0], X_data.shape[1], 1))\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(units=50, return_sequences=True, input_shape=(seq_length, 1)),\n",
        "    LSTM(units=50, return_sequences=False),\n",
        "    Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(X_data, y_data, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Forecast the next 24 months\n",
        "last_sequence = data_scaled[-seq_length:]\n",
        "forecast = []\n",
        "for _ in range(24):\n",
        "    prediction = model.predict(last_sequence.reshape(1, seq_length, 1))[0][0]\n",
        "    forecast.append(prediction)\n",
        "    last_sequence = np.append(last_sequence[1:], prediction).reshape(seq_length, 1)\n",
        "\n",
        "# Inverse the scaling\n",
        "forecast = scaler.inverse_transform(np.array(forecast).reshape(-1, 1)).flatten()\n",
        "\n",
        "# Generate forecast dates\n",
        "forecast_dates = pd.date_range(start=data['date'].iloc[-1], periods=25, freq='M')[1:]\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(data['date'], data['data'], color='blue', label='Actual Data')\n",
        "plt.plot(forecast_dates, forecast, linestyle='--', color='orange', label='Forecasted Data')\n",
        "plt.axvline(x=data['date'].iloc[-1], color='gray', linestyle='--', label='Start of Forecast')\n",
        "plt.title('Johnson & Johnson Sales Data and Forecast (LSTM)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C_h9rv4p8qRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRU**"
      ],
      "metadata": {
        "id": "BLo436F39Gfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "\n",
        "# Load data\n",
        "file_path = \"/content/drive/MyDrive/jj.csv\"\n",
        "data = pd.read_csv(file_path, parse_dates=['date'])\n",
        "data.sort_values(by='date', inplace=True)\n",
        "\n",
        "# Scale the data\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data['data'].values.reshape(-1, 1))\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:(i + seq_length), 0])\n",
        "        y.append(data[i + seq_length, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 12\n",
        "X_data, y_data = create_sequences(data_scaled, seq_length)\n",
        "\n",
        "# Reshape data for GRU\n",
        "X_data = X_data.reshape((X_data.shape[0], X_data.shape[1], 1))\n",
        "\n",
        "# Build the GRU model\n",
        "model = Sequential([\n",
        "    GRU(units=50, return_sequences=True, input_shape=(seq_length, 1)),\n",
        "    GRU(units=50, return_sequences=False),\n",
        "    Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(X_data, y_data, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Forecast the next 24 months\n",
        "last_sequence = data_scaled[-seq_length:]\n",
        "forecast = []\n",
        "for _ in range(24):\n",
        "    prediction = model.predict(last_sequence.reshape(1, seq_length, 1))[0][0]\n",
        "    forecast.append(prediction)\n",
        "    last_sequence = np.append(last_sequence[1:], prediction).reshape(seq_length, 1)\n",
        "\n",
        "# Inverse the scaling\n",
        "forecast = scaler.inverse_transform(np.array(forecast).reshape(-1, 1)).flatten()\n",
        "\n",
        "# Generate forecast dates\n",
        "forecast_dates = pd.date_range(start=data['date'].iloc[-1], periods=25, freq='M')[1:]\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(data['date'], data['data'], color='blue', label='Actual Data')\n",
        "plt.plot(forecast_dates, forecast, linestyle='--', color='orange', label='Forecasted Data')\n",
        "plt.axvline(x=data['date'].iloc[-1], color='gray', linestyle='--', label='Start of Forecast')\n",
        "plt.title('Johnson & Johnson Sales Data and Forecast (GRU)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9LAMa1t_9E6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate Both Models**"
      ],
      "metadata": {
        "id": "gjqgHskl9Vab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "def evaluate_model(actual, forecast):\n",
        "    mae = mean_absolute_error(actual, forecast)\n",
        "    mse = mean_squared_error(actual, forecast)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mpe = np.mean((actual - forecast) / actual) * 100\n",
        "    mape = np.mean(np.abs((actual - forecast) / actual)) * 100\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'Metric': ['Mean Absolute Error (MAE)', 'Mean Squared Error (MSE)', 'Root Mean Squared Error (RMSE)',\n",
        "                   'Mean Percentage Error (MPE)', 'Mean Absolute Percentage Error (MAPE)'],\n",
        "        'Value': [mae, mse, rmse, mpe, mape]\n",
        "    })\n",
        "\n",
        "# Actual data for evaluation\n",
        "actual_data = data['data'].values[-24:]\n",
        "\n",
        "# Evaluation for LSTM\n",
        "lstm_evaluation = evaluate_model(actual_data, forecast)\n",
        "\n",
        "# Evaluation for GRU\n",
        "gru_evaluation = evaluate_model(actual_data, forecast)\n",
        "\n",
        "print(\"LSTM Evaluation:\")\n",
        "print(lstm_evaluation)\n",
        "\n",
        "print(\"\\nGRU Evaluation:\")\n",
        "print(gru_evaluation)\n"
      ],
      "metadata": {
        "id": "cLZ_rxOK9aVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Pre trained models**"
      ],
      "metadata": {
        "id": "yUbWbOy29peE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Facebook Prophet**"
      ],
      "metadata": {
        "id": "yRidOMoc9vr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install prophet\n"
      ],
      "metadata": {
        "id": "a67kClPQ9vGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from prophet import Prophet\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "file_path = \"/content/drive/MyDrive/jj.csv\"\n",
        "data = pd.read_csv(file_path, parse_dates=['date'])\n",
        "data = data.rename(columns={'date': 'ds', 'data': 'y'})\n",
        "\n",
        "# Fit the model\n",
        "model = Prophet()\n",
        "model.fit(data)\n",
        "\n",
        "# Make a future dataframe for 24 months\n",
        "future = model.make_future_dataframe(periods=24, freq='M')\n",
        "forecast = model.predict(future)\n",
        "\n",
        "# Plot the forecast\n",
        "fig = model.plot(forecast)\n",
        "plt.title('Johnson & Johnson Sales Forecast with Prophet')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()\n",
        "\n",
        "# Plot the forecast components\n",
        "fig2 = model.plot_components(forecast)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Jowt3uyv9pGT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}